---
title: "Project"
author: "Victoria (Tori) Widjaja & Jeremiah Fa'atiliga"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r libraries, warning=FALSE, message=FALSE}
#install.packages("gt")
#install.packages("skimr")
#install.packages("gtsummary")
#install.packages("GGally")
#install.packages("smotefamily")
#install.packages("gbm")

# Core Data Manipulation Packages
library(dplyr)
library(tidyr)
library(tibble)

# Data Import Packages
library(readr)

# Tidyverse (loads ggplot2, dplyr, purrr, etc.)
library(tidyverse)

# Visualization Packages
library(ggplot2)
library(GGally)
library(corrplot)
library(plotmo)

# Modeling Packages
library(caret)
library(kernlab)
library(earth)
library(gbm)

# EDA Packages
library(skimr)
library(psych)
library(reshape2)
library(gtsummary)

# Additional Packages
library(gt)

# Commented out packages if not needed
# library(patchwork)
# library(AppliedPredictiveModeling)
# library(randomForest)


seed <- 123
```

```{r loading, warning=FALSE, message=FALSE}

dir_prefix <- getwd()
print(dir_prefix)

### Connection info for GitHub File
url <- paste(dir_prefix, 'healthcare-dataset-stroke-data.csv', sep ='/')
df_orig <- read_csv(url)
print(url)

describe(df_orig)
```

## Exploratory Data Analysis (EDA)

```{r}
###graphical and non-graphical representations of relationships between the response variable and predictor variables

df_eda <- df_orig

rownames(df_eda) <- df_eda$id
df_eda <- dplyr::select(df_eda, -id)

print(df_eda)
```

words words words

```{r warning=FALSE, message=FALSE}

# Filter out N/A values for bmi and convert to numeric
df_eda <- df_eda %>%
  filter(!is.na(bmi)) %>%
  mutate(bmi = as.numeric(bmi))

# Pivot longer and convert value column to numeric if possible
df_long <- df_eda %>%
  pivot_longer(-c(stroke, ever_married, gender, hypertension, heart_disease, Residence_type, work_type, smoking_status), names_to = "Variable", values_to = "value") %>%
  mutate(value = as.numeric(value))

# Plot histograms
ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 20) +
  facet_wrap(~Variable, scales = "free", ncol = 2) + 
  theme_bw() +
  labs(title = 'Histograms of Variables Distributions', x = NULL, y = "Count")

```

```{r warning=FALSE, message=FALSE, fig.width=7,fig.height=6}

numeric_df <- df_eda[sapply(df_eda, is.numeric)]

numeric_df |>
  ggpairs(title = "Relationship Between Predictors", progress = FALSE)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}

stroke_percent <- prop.table(table(df_eda$stroke)) * 100

barplot(stroke_percent, 
        main = "Percentage of Strokes", 
        xlab = "Stroke", 
        ylab = "Percentage",
        col = "lightblue",
        ylim = c(0, max(stroke_percent) + 11),
        names.arg = c("No Stroke", "Stroke"))

label_pos <- stroke_percent + 1

### Add labels
text(x = 1:length(stroke_percent), 
     y = label_pos, 
     labels = paste0(round(stroke_percent, 2), "%"), 
     pos = 3)

```

```{r}

wrap_text <- function(x, width) {
  sapply(strwrap(x, width = width, simplify = FALSE), paste, collapse = "\n")
}

# Select non-numeric columns
non_numeric_columns <- names(df_eda)[sapply(df_eda, is.factor) | sapply(df_eda, is.character)]

# Function to create plots
create_plots <- function(col) {
  counts <- table(df_eda[[col]], df_eda$stroke)
  counts_df <- as.data.frame(counts)
  names(counts_df) <- c(col, "stroke", "count")
  
  ggplot(counts_df, aes(x = !!sym(col), y = count, fill = factor(stroke))) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_text(aes(label = paste0(count)),
              vjust = -0.5, size = 3, position = position_dodge(width = 0.9)) +  # Add count / percent labels
    labs(x = col, y = "Count of Records", 
         title = paste("Bar Graph of Stroke by", col, " by Count")) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          panel.spacing = unit(3, "lines"),
          legend.text = element_text(size = 7),  # Adjust legend text size
          legend.title = element_text(size = 9))  # Adjust legend title size
}

# Create plots for each non-numeric column
plots_list <- map(non_numeric_columns, ~ create_plots(.x))

# Print plots
walk(plots_list, print)


```

```{r}

# Function to create plots
create_plots <- function(col) {
  counts <- table(df_eda[[col]], df_eda$stroke)
  counts_df <- as.data.frame(counts)
  names(counts_df) <- c(col, "stroke", "count")
  
  # Calculate percentages within each category
  counts_df <- counts_df %>%
    group_by(stroke) %>%
    mutate(percent = count / sum(count) * 100) %>%
    ungroup()
  
  ggplot(counts_df, aes(x = !!sym(col), y = percent, fill = factor(stroke))) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = paste0(round(percent, 1), "%")),
              vjust = -0.5, size = 3) +  # Add percent labels
    labs(x = col, y = "Percentage of Records", 
         title = paste("Bar Graph of Stroke by", col, "\nPercentage (Total Stroke Type %)")) +
    facet_wrap(~stroke, scales = "free") +  # Facet by stroke
    theme(axis.text.x = element_text(angle = 30, hjust = 1),
          panel.spacing = unit(3, "lines")
)}

# Create plots for each non-numeric column
plots_list <- map(non_numeric_columns, ~ create_plots(.x))

# Print plots
walk(plots_list, print)

```

```{r}
##### HEAT MAP #### For Feature Selection ################################
# Numeric Columns for EDA
heatmap_data <- df_eda %>%
  select(age, hypertension, heart_disease, avg_glucose_level, bmi, stroke)

# Correlation Matrix
correlation_matrix <- cor(heatmap_data, use = "pairwise.complete.obs")

# Heatmap Plot
ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "Light Blue", high = "Navy Blue") +
  labs(x = "Features", y = "Features", title = "Stroke Heatmap") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Data Splitting - Training & Test

```{r}
###training, validation, and test sets   
###Split  
trainIndex <- createDataPartition(df_eda$stroke, p = 0.8, ### saving 20% for test
                                  list = FALSE,
                                  times = 1)   

# Subset data into training and testing sets  
trainData <- df_eda[trainIndex, ]  
testData <- df_eda[-trainIndex, ]

```

## Data Wrangling and Pre-Processing

```{r}
# Check for missing data
colSums(is.na(trainData))

### bagImpute used because missing data is random
bag_missing <- preProcess(trainData, method = "bagImpute")  ##### CHANGE? knn ??
trainData <- predict(bag_missing, newdata = trainData)

# bagImpute for Test Data
bag_missing_test <- preProcess(trainData, method = "bagImpute")  ##### CHANGE? knn ??
testData <- predict(bag_missing_test, newdata = testData)

colSums(is.na(trainData))

```

```{r}
### Factor 
non_numeric_cols <- sapply(trainData, function(x) !is.numeric(x))  
# Convert non-numeric columns to factors 
trainData <- trainData %>%   
  mutate_if(non_numeric_cols, as.factor) 

### Factor Test 
non_numeric_cols <- sapply(testData, function(x) !is.numeric(x))  
# Convert non-numeric columns to factors Test
testData <- testData %>%   
  mutate_if(non_numeric_cols, as.factor) 

str(trainData)

```

```{r}
### dummy variables for factors
dummy_model1 <- dummyVars(stroke ~ gender + ever_married + work_type + Residence_type + smoking_status, data = trainData)

trainData_dummy <- as.data.frame(predict(dummy_model1, newdata = trainData))
trainData_dummy <- as.data.frame(lapply(trainData_dummy, as.factor))

### dummy variables for test factors
dummy_model2 <- dummyVars(stroke ~ gender + ever_married + work_type + Residence_type + smoking_status, data = testData)

testData_dummy <- as.data.frame(predict(dummy_model2, newdata = testData))
testData_dummy <- as.data.frame(lapply(testData_dummy, as.factor))

str(trainData_dummy)
```

```{r}
### Dropping columns that can be inferred from the others to avoid multicollinearity 
trainData_dummy <-select(trainData_dummy, -gender.Female, -ever_married.No, -work_type.children, -Residence_type.Rural, -smoking_status.Unknown)

str(trainData_dummy)
trainData_selected <- select(trainData, age, hypertension, heart_disease, avg_glucose_level, bmi, stroke)
str(trainData_selected)

### Test Dropping columns that can be inferred from the others to avoid multicollinearity 
testData_dummy <-select(testData_dummy, -gender.Female, -ever_married.No, -work_type.children, -Residence_type.Rural, -smoking_status.Unknown)
testData_selected <- select(testData, age, hypertension, heart_disease, avg_glucose_level, bmi, stroke)
```

```{r}
# Train
trainData_ready <- cbind(trainData_dummy, trainData_selected)
trainData_ready <- trainData_ready %>%
  mutate(across(all_of(c("hypertension", "heart_disease", "stroke")), as.factor))
names(trainData_ready) <- gsub("\\.", "_", names(trainData_ready))
names(trainData_ready) <- gsub("_Yes", "", names(trainData_ready))
trainData_ready$stroke <- as.factor(make.names(as.character(trainData_ready$stroke)))
str(trainData_ready)

# Test
testData_ready <- cbind(testData_dummy, testData_selected)
testData_ready <- testData_ready %>%
  mutate(across(all_of(c("hypertension", "heart_disease", "stroke")), as.factor))
names(testData_ready) <- gsub("\\.", "_", names(testData_ready))
names(testData_ready) <- gsub("_Yes", "", names(testData_ready))
testData_ready$stroke <- as.factor(make.names(as.character(testData_ready$stroke)))
```

```{r}
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10,  
                     repeats = 5,  # Increase to 5 as per your requirement
                     verboseIter = FALSE,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

```

```{r}
### Base Model Logistic Regression Model

# Fit logistic regression model
lrm_base_model <- train(stroke ~ ., 
                        data = trainData_ready, 
                        method = "glm", 
                        family = "binomial",
                        trControl = ctrl)

# View model summary
summary(lrm_base_model)

```

```{r}

```

## Feature Selection/Engineering

```{r}



```

```{r}



```

## Data Prep

```{r}
# Exclude stroke and Identify and remove variables with zero variance
Zero_Var_vars <- nearZeroVar(trainData_ready[, -which(names(trainData_ready) == "stroke"), drop = FALSE])
trainData_ready <- trainData_ready[, -Zero_Var_vars, drop = FALSE]

Zero_Var_vars2 <- nearZeroVar(testData_ready[, -which(names(testData_ready) == "stroke"), drop = FALSE])
testData_ready <- testData_ready[, -Zero_Var_vars2, drop = FALSE]

# Structure of Train Data
str(trainData_ready)

# Set up control parameters for training
ctrl <- trainControl(method = 'repeatedcv', 
                     repeats = 1, ### change when ready for final run to 5
                     number = 5, 
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)
```

## Models

```{r}
###model tuning and evaluation per and sum at the end
```

### Model #1 Baseline

```{r}
rseed <- 123
set.seed(rseed)
# Linear Discriminant Analysis
ldaFit_stroke <- train(stroke ~ ., 
                    data = trainData_ready,
                    method = 'lda',
                    preProc = c("center","scale"),
                    metric = 'ROC',
                    trControl = ctrl)
ldaFit_stroke
ldaCM_stroke <- confusionMatrix(ldaFit_stroke, norm = "none")
ldaCM_stroke
```

### Model #2

```{r}
set.seed(rseed)

glmnGrid <- expand.grid(alpha = c( .5, .6, 0.7),
lambda = seq(.01, .2, length = 20))

# Penalized Logistic Regression
plrFit_stroke <- train(stroke ~ ., 
                    data = trainData_ready,
                    method = 'glmnet',
                    tuneGrid = glmnGrid,
                    preProc = c('center', 'scale'),
                    metric = 'ROC',
                    trControl = ctrl)

plot(plrFit_stroke)

# Best tuning parameters
optimal_plr_tune <- plrFit_stroke$bestTune
print(paste('Best Alpha and Lambda tuning parameters for Penalized Logistic Regression:', paste(optimal_plr_tune, collapse = ',')))

```

### Model #3

```{r}
set.seed(rseed)
nscGrid <- data.frame(threshold = seq(2, 8.5, length = 20))


# Nearest Shrunken Centroids
nscFit_stroke <- train(stroke ~ ., 
                    data = trainData_ready,
                    method = 'pam',
                    preProc = c('center', 'scale'),
                    tuneGrid = nscGrid,
                    metric = 'ROC',
                    trControl = ctrl)
plot(nscFit_stroke)

# Best tuning parameters
optimal_nsc_tune <- nscFit_stroke$bestTune
print(paste('Best threshold tuning parameter for Nearest Shrunken Centroids:', paste(optimal_nsc_tune, collapse = ',')))
```

### Model #4

```{r}
set.seed(rseed)

grid <- expand.grid(size = c(1, 2, 3),     # Number of hidden units
                    decay = c(0.05, 0.06, 0.07, 0.08))  # Weight decay

nnFit_stroke <- train(stroke ~ .,
                      data = trainData_ready,
                      method = 'nnet',
                      maxit = 500,
                      preProcess = c('center', 'scale'),  
                      metric = 'ROC',    
                      trControl = ctrl,
                      tuneGrid = grid, 
                      trace = FALSE)
plot(nnFit_stroke)

# Best tuning parameters
optimal_nn_tune <- nnFit_stroke$bestTune
print(paste('Best Size and Decay tuning parameters for Neural Network:', paste(optimal_nn_tune, collapse = ',')))

```

### Model #5

```{r}
library(caret)

# Set seed for reproducibility
set.seed(rseed)


rf_grid <- expand.grid(
  mtry = c(9, 10, 11) 
)

rfFit_stroke <- train(
  stroke ~ .,                   
  data = trainData_ready, 
  method = "rf", 
  trControl = ctrl,
  tuneGrid = rf_grid
)

# Print the best parameters and performance metrics
print(rfFit_stroke)

```

### Results - Summary Table

```{r}
###use gt() table for all results
# Make predictions for Random Forest
rf_pred <- predict(rfFit_stroke, newdata = testData_ready)
rf_cm <- confusionMatrix(rf_pred, testData_ready$stroke)

# Make predictions for Linear Discriminant Analysis
lda_pred <- predict(ldaFit_stroke, newdata = testData_ready)
lda_cm <- confusionMatrix(lda_pred, testData_ready$stroke)

# Make predictions for Penalized Logistic Regression
plr_pred <- predict(plrFit_stroke, newdata = testData_ready)
plr_cm <- confusionMatrix(plr_pred, testData_ready$stroke)

# Make predictions for Nearest Shrunken Centroids
nsc_pred <- predict(nscFit_stroke, newdata = testData_ready)
nsc_cm <- confusionMatrix(nsc_pred, testData_ready$stroke)

# Make predictions for Neural Network
nn_pred <- predict(nnFit_stroke, newdata = testData_ready)
nn_cm <- confusionMatrix(nn_pred, testData_ready$stroke)

extract_accuracy <- function(model, cm) {
  tibble(
    model = model,
    Accuracy = cm$overall['Accuracy'],
    `CI Lower` = cm$overall['AccuracyLower'],
    `CI Upper` = cm$overall['AccuracyUpper']
  )
}

# Combine accuracies into a table
accuracies <- bind_rows(
  extract_accuracy('Random Forest', rf_cm),
  extract_accuracy('Linear Discriminant Analysis', lda_cm),
  extract_accuracy('Penalized Logistic Regression', plr_cm),
  extract_accuracy('Nearest Shrunken Centroids', nsc_cm),
  extract_accuracy('Neural Network', nn_cm)
)

# Display the table using gt package
accuracies %>%
  arrange(-Accuracy) %>%
  gt() %>%
  fmt_number(columns = c(Accuracy, `CI Lower`, `CI Upper`), decimals = 3)

```

### Tuned Models

```{r}
# Penalized Logistic Regression with optimal parameters
plrFit_stroke2 <- train(stroke ~ ., 
                       data = trainData_ready,
                       method = 'glmnet',
                       preProc = c('center', 'scale'),
                       metric = 'ROC',
                       trControl = ctrl,
                       tuneGrid = expand.grid(
                         alpha = optimal_plr_tune$alpha,
                         lambda = optimal_plr_tune$lambda
                       ))

# Retrain NSC with optimal parameters
nscFit_stroke2 <- train(stroke ~ ., 
                        data = trainData_ready,
                        method = 'pam',
                        preProc = c('center', 'scale'),
                        metric = 'ROC',
                        trControl = ctrl,
                        tuneGrid = expand.grid(
                          threshold = optimal_nsc_tune$threshold
                        ))

# Retrain NN with optimal parameters
nnFit_stroke2 <- train(stroke ~ .,
                       data = trainData_ready,
                       method = 'nnet',
                       maxit = 1000,
                       preProcess = c('center', 'scale'),  
                       metric = 'ROC',    
                       trControl = ctrl,
                       tuneGrid = expand.grid(
                         size = optimal_nn_tune$size,
                         decay = optimal_nn_tune$decay
                         ),
                       trace = FALSE)
```

### Summary for Tuned Models

```{r}
# Make predictions for Linear Discriminant Analysis
lda_pred <- predict(ldaFit_stroke, newdata = testData_ready)
lda_cm2 <- confusionMatrix(lda_pred, testData_ready$stroke)

# Make predictions for Penalized Logistic Regression
plr_pred2 <- predict(plrFit_stroke2, newdata = testData_ready)
plr_cm2 <- confusionMatrix(plr_pred2, testData_ready$stroke)

# Make predictions for Nearest Shrunken Centroids
nsc_pred2 <- predict(nscFit_stroke2, newdata = testData_ready)
nsc_cm2 <- confusionMatrix(nsc_pred2, testData_ready$stroke)

# Make predictions for Neural Network
nn_pred2 <- predict(nnFit_stroke2, newdata = testData_ready)
nn_cm2 <- confusionMatrix(nn_pred2, testData_ready$stroke)

extract_accuracy <- function(model, cm) {
  tibble(
    model = model,
    Accuracy = cm$overall['Accuracy'],
    `CI Lower` = cm$overall['AccuracyLower'],
    `CI Upper` = cm$overall['AccuracyUpper']
  )
}

# Combine accuracies into a table
accuracies <- bind_rows(
  extract_accuracy('Linear Discriminant Analysis', lda_cm2),
  extract_accuracy('Penalized Logistic Regression', plr_cm2),
  extract_accuracy('Nearest Shrunken Centroids', nsc_cm2),
  extract_accuracy('Neural Network', nn_cm2)
)

# Display the table using gt package
accuracies %>%
  arrange(-Accuracy) %>%
  gt() %>%
  fmt_number(columns = c(Accuracy, `CI Lower`, `CI Upper`), decimals = 3)
```
