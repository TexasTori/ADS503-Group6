---
title: "Project"
author: "Victoria (Tori) Widjaja & Jeremiah Fa'atiliga"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---


## R Markdown

```{r libraries, warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(tibble)
library(readr)
library(tidyverse)
library(ggplot2)
library(GGally)
library(corrplot)
library(plotmo)
library(caret)
library(kernlab)
library(earth)
library(skimr)
library(psych)
library(reshape2)
library(gt)
library(ROSE)


rseed <- 123
```

```{r loading, warning=FALSE, message=FALSE}

dir_prefix <- getwd()
print(dir_prefix)

### Connection info for GitHub File
url <- paste(dir_prefix, 'healthcare-dataset-stroke-data.csv', sep ='/')
df_orig <- read_csv(url)
print(url)

describe(df_orig)
```

## Exploratory Data Analysis (EDA)

```{r}
###graphical and non-graphical representations of relationships between the response variable and predictor variables

df_eda <- df_orig

rownames(df_eda) <- df_eda$id
df_eda <- dplyr::select(df_eda, -id)

print(df_eda)
```

### Histograms

```{r warning=FALSE, message=FALSE}

# Filter out N/A values for bmi and convert to numeric
df_eda <- df_eda %>%
  filter(!is.na(bmi)) %>%
  mutate(bmi = as.numeric(bmi))

# Pivot longer and convert value column to numeric if possible
df_long <- df_eda %>%
  pivot_longer(-c(stroke, ever_married, gender, hypertension, heart_disease, Residence_type, work_type, smoking_status), names_to = "Variable", values_to = "value") %>%
  mutate(value = as.numeric(value))

# Plot histograms
ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 20) +
  facet_wrap(~Variable, scales = "free", ncol = 2) + 
  theme_bw() +
  labs(title = 'Histograms of Variables Distributions', x = NULL, y = "Count")

```

### Corrplot

```{r warning=FALSE, message=FALSE, fig.width=7,fig.height=6}

numeric_df <- df_eda[sapply(df_eda, is.numeric)]

numeric_df |>
  ggpairs(title = "Relationship Between Predictors", progress = FALSE)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Class Balance

```{r}

stroke_percent <- prop.table(table(df_eda$stroke)) * 100

barplot(stroke_percent, 
        main = "Percentage of Strokes", 
        xlab = "Stroke", 
        ylab = "Percentage",
        col = "lightblue",
        ylim = c(0, max(stroke_percent) + 11),
        names.arg = c("No Stroke", "Stroke"))

label_pos <- stroke_percent + 1

### Add labels
text(x = 1:length(stroke_percent), 
     y = label_pos, 
     labels = paste0(round(stroke_percent, 2), "%"), 
     pos = 3)

```

### Distributions of Values (Counts)

```{r}

wrap_text <- function(x, width) {
  sapply(strwrap(x, width = width, simplify = FALSE), paste, collapse = "\n")
}

# Select non-numeric columns
non_numeric_columns <- names(df_eda)[sapply(df_eda, is.factor) | sapply(df_eda, is.character)]

# Function to create plots
create_plots <- function(col) {
  counts <- table(df_eda[[col]], df_eda$stroke)
  counts_df <- as.data.frame(counts)
  names(counts_df) <- c(col, "stroke", "count")
  
  ggplot(counts_df, aes(x = !!sym(col), y = count, fill = factor(stroke))) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_text(aes(label = paste0(count)),
              vjust = -0.5, size = 3, position = position_dodge(width = 0.9)) +  # Add count / percent labels
    labs(x = col, y = "Count of Records", 
         title = paste("Bar Graph of Stroke by", col, " by Count")) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          panel.spacing = unit(3, "lines"),
          legend.text = element_text(size = 7),  # Adjust legend text size
          legend.title = element_text(size = 9))  # Adjust legend title size
}

# Create plots for each non-numeric column
plots_list <- map(non_numeric_columns, ~ create_plots(.x))

# Print plots
walk(plots_list, print)


```

### Distributions of Values (Percentages)

```{r}

# Function to create plots
create_plots <- function(col) {
  counts <- table(df_eda[[col]], df_eda$stroke)
  counts_df <- as.data.frame(counts)
  names(counts_df) <- c(col, "stroke", "count")
  
  # Calculate percentages within each category
  counts_df <- counts_df %>%
    group_by(stroke) %>%
    mutate(percent = count / sum(count) * 100) %>%
    ungroup()
  
  ggplot(counts_df, aes(x = !!sym(col), y = percent, fill = factor(stroke))) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = paste0(round(percent, 1), "%")),
              vjust = -0.5, size = 3) +  # Add percent labels
    labs(x = col, y = "Percentage of Records", 
         title = paste("Bar Graph of Stroke by", col, "\nPercentage (Total Stroke Type %)")) +
    facet_wrap(~stroke, scales = "free") +  # Facet by stroke
    theme(axis.text.x = element_text(angle = 30, hjust = 1),
          panel.spacing = unit(3, "lines")
)}

# Create plots for each non-numeric column
plots_list <- map(non_numeric_columns, ~ create_plots(.x))

# Print plots
walk(plots_list, print)

```

### Heatmap

```{r}
##### HEAT MAP #### For Feature Selection ################################
# Numeric Columns for EDA
heatmap_data <- df_eda %>%
  select(age, hypertension, heart_disease, avg_glucose_level, bmi, stroke)

# Correlation Matrix
correlation_matrix <- cor(heatmap_data, use = "pairwise.complete.obs")

# Heatmap Plot
ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "Light Blue", high = "Navy Blue") +
  labs(x = "Features", y = "Features", title = "Stroke Heatmap") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Data Splitting - Training & Test

```{r}
###training, validation, and test sets   
###Split  
trainIndex <- createDataPartition(df_eda$stroke, p = 0.8, ### saving 20% for test
                                  list = FALSE,
                                  times = 1)   

# Subset data into training and testing sets  
trainData <- df_eda[trainIndex, ]  
testData <- df_eda[-trainIndex, ]

```

## Data Wrangling and Pre-Processing

### Missing Values

```{r}
table(trainData$gender)

trainData$gender[trainData$gender == "Other"] <- {
  ux <- unique(trainData$gender)
  mode_gender <- ux[which.max(tabulate(match(trainData$gender, ux)))]
  mode_gender
}

testData$gender[testData$gender == "Other"] <- {
  ux <- unique(testData$gender)
  mode_gender <- ux[which.max(tabulate(match(testData$gender, ux)))]
  mode_gender
}
table(trainData$gender)

# Check for missing data
colSums(is.na(trainData)) 

### bagImpute used because missing data is random
bag_missing <- preProcess(trainData, method = "bagImpute")  ##### CHANGE? knn ??
trainData <- predict(bag_missing, newdata = trainData)

# bagImpute for Test Data
testData <- predict(bag_missing, newdata = testData)

colSums(is.na(trainData))

```

### Setting up Factors

```{r factor}
### Factor Train
non_numeric_cols <- sapply(trainData, function(x) !is.numeric(x))  
# Convert non-numeric columns to factors 
trainData <- trainData %>%   
  mutate_if(non_numeric_cols, as.factor) 

### Factor Test 
non_numeric_cols <- sapply(testData, function(x) !is.numeric(x))  
# Convert non-numeric columns to factors Test
testData <- testData %>%   
  mutate_if(non_numeric_cols, as.factor) 

#str(trainData)
#str(testData)

```

### Dummy Variables

```{r dummy}
### dummy variables for Trfactors
dummy_model1 <- dummyVars(stroke ~ gender + ever_married + work_type + Residence_type + smoking_status, data = trainData)

trainData_dummy <- as.data.frame(predict(dummy_model1, newdata = trainData))
trainData_dummy <- as.data.frame(lapply(trainData_dummy, as.factor))

### dummy variables for test factors
dummy_model2 <- dummyVars(stroke ~ gender + ever_married + work_type + Residence_type + smoking_status, data = testData)

testData_dummy <- as.data.frame(predict(dummy_model2, newdata = testData))
testData_dummy <- as.data.frame(lapply(testData_dummy, as.factor))

#head(trainData_dummy)
#head(testData_dummy)
```

```{r}
### Dropping columns that can be inferred from the others to avoid multicollinearity 
trainData_dummy <-select(trainData_dummy, -gender.Female, -ever_married.No, -work_type.children, -Residence_type.Rural, -smoking_status.Unknown)

#head(trainData_dummy)
trainData_selected <- select(trainData, age, hypertension, heart_disease, avg_glucose_level, bmi, stroke)
#str(trainData_selected)

### Test Dropping columns that can be inferred from the others to avoid multicollinearity 
testData_dummy <-select(testData_dummy, -gender.Female, -ever_married.No, -work_type.children, -Residence_type.Rural, -smoking_status.Unknown)
testData_selected <- select(testData, age, hypertension, heart_disease, avg_glucose_level, bmi, stroke)
#str(testData_dummy)
```

```{r combo}
# Train
trainData_ready <- cbind(trainData_dummy, trainData_selected)
trainData_ready <- trainData_ready %>%
  mutate(across(all_of(c("hypertension", "heart_disease", "stroke")), as.factor))
names(trainData_ready) <- gsub("\\.", "_", names(trainData_ready))
names(trainData_ready) <- gsub("_Yes", "", names(trainData_ready))
trainData_ready$stroke <- as.factor(make.names(as.character(trainData_ready$stroke)))
head(trainData_ready)

# Test
testData_ready <- cbind(testData_dummy, testData_selected)
testData_ready <- testData_ready %>%
  mutate(across(all_of(c("hypertension", "heart_disease", "stroke")), as.factor))
names(testData_ready) <- gsub("\\.", "_", names(testData_ready))
names(testData_ready) <- gsub("_Yes", "", names(testData_ready))
testData_ready$stroke <- as.factor(make.names(as.character(testData_ready$stroke)))
str(testData_ready)
```

### Class Imbalance (ROSE Method)

```{r}
# Calculate counts and convert to data frame
stroke_counts <- as.data.frame(table(trainData_ready$stroke))
colnames(stroke_counts) <- c("Stroke", "Count")
stroke_counts$Stroke <- factor(stroke_counts$Stroke, levels = c('X0', 'X1'), labels = c("No Stroke", "Stroke"))

### Bar Chart
ggplot(stroke_counts, aes(x = Stroke, y = Count)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Count of Stroke Values",
       x = "Stroke",
       y = "Count") +
  geom_text(aes(label = Count), 
            vjust = -0.5,  # Position above the bars
            size = 3,  # Size of the text
            color = "black")  # Color of the text
```

```{r}
set.seed(rseed)

# Apply ROSE to balance the dataset
rose_train <- ROSE(stroke ~ ., data = trainData_ready)$data

# Separate predictors (features) and target variable (stroke)
train_X <- rose_train[, !(names(rose_train) %in% "stroke")]
train_y <- rose_train$stroke

train_y_df <- data.frame(stroke = train_y)

stroke_counts <- as.data.frame(table(train_y_df$stroke))
colnames(stroke_counts) <- c("Stroke", "Count")
stroke_counts$Stroke <- factor(stroke_counts$Stroke, 
                               levels = c('X0', 'X1'), labels = c("No Stroke", "Stroke"))
### Bar chart
ggplot(stroke_counts, aes(x = Stroke, y = Count)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Count of Stroke Values in Balanced Dataset",
       x = "Stroke",
       y = "Count") +
  geom_text(aes(label = Count), 
            vjust = -0.5,  
            size = 3,  
            color = "black")  

trainData_ready <- data.frame(rose_train)
```

### Baseline Model to Beat

```{r}
set.seed(123)
### Parameters for Tuning
ctrl <- trainControl(method = "repeatedcv", 
                     number = 5,  
                     repeats = 3,  
                     verboseIter = FALSE,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

```

```{r}

### Base Model Logistic Regression Model

# Fit logistic regression model
lrm_base_model <- train(stroke ~ ., 
                        data = trainData_ready, 
                        method = "glm", 
                        family = "binomial",
                        trControl = ctrl)

# View model summary
summary(lrm_base_model)

```

## Data Prep

```{r}

# Exclude stroke and Identify and remove variables with zero variance
Zero_Var_vars <- nearZeroVar(trainData_ready[, -which(names(trainData_ready) == "stroke"), drop = FALSE])
trainData_ready <- trainData_ready[, -Zero_Var_vars, drop = FALSE]
print(names(Zero_Var_vars))
```

## Models

### Model #1 - Linear Discriminant Analysis (LDR)

```{r m1}

rseed <- 123
set.seed(rseed)
# Linear Discriminant Analysis
ldaFit_stroke <- train(stroke ~ ., 
                    data = trainData_ready,
                    method = 'lda',
                    preProc = c("center","scale"),
                    metric = 'ROC',
                    trControl = ctrl)
ldaFit_stroke
ldaCM_stroke <- confusionMatrix(ldaFit_stroke, norm = "none")
ldaCM_stroke
```

### Model #2 - Penalized Logistic Regression (PLR)

```{r m2}

set.seed(rseed)

glmnGrid <- expand.grid(alpha = c( 0, .5, 1),
lambda = seq(.001, 2, length = 20))

# Penalized Logistic Regression
plrFit_stroke <- train(stroke ~ ., 
                    data = trainData_ready,
                    method = 'glmnet',
                    tuneGrid = glmnGrid,
                    preProc = c('center', 'scale'),
                    metric = 'ROC',
                    trControl = ctrl)

plot(plrFit_stroke)

# Best tuning parameters
optimal_plr_tune <- plrFit_stroke$bestTune
print(paste('Best Alpha and Lambda tuning parameters for Penalized Logistic Regression:', paste(optimal_plr_tune, collapse = ',')))

```

### Model #3 - Nearest Shrunken Centroids

```{r m3}

set.seed(rseed)
nscGrid <- data.frame(threshold = seq(2, 7, length = 20))


# Nearest Shrunken Centroids
nscFit_stroke <- train(stroke ~ ., 
                    data = trainData_ready,
                    method = 'pam',
                    preProc = c('center', 'scale'),
                    tuneGrid = nscGrid,
                    metric = 'ROC',
                    trControl = ctrl)
plot(nscFit_stroke)

# Best tuning parameters
optimal_nsc_tune <- nscFit_stroke$bestTune
print(paste('Best threshold tuning parameter for Nearest Shrunken Centroids:', paste(optimal_nsc_tune, collapse = ',')))
```

### Model #4 - Neural Network

```{r m4}

set.seed(rseed)

grid <- expand.grid(size = c(1, 5, 8),     # Number of hidden units
                    decay = c( 0.01, 0.05, .1))  # Weight decay
# Neural Networks
nnFit_stroke <- train(stroke ~ .,
                      data = trainData_ready,
                      method = 'nnet',
                      maxit = 500,
                      preProcess = c('center', 'scale'),  
                      metric = 'ROC',    
                      trControl = ctrl,
                      tuneGrid = grid, 
                      trace = FALSE)
plot(nnFit_stroke)

# Best tuning parameters
optimal_nn_tune <- nnFit_stroke$bestTune
print(paste('Best Size and Decay tuning parameters for Neural Network:', paste(optimal_nn_tune, collapse = ',')))

```

### Model #5 - Random Forest

```{r m5}

# Set seed for reproducibility
set.seed(rseed)

# Random Forest
rf_grid <- expand.grid(
  mtry = c(2, 4, 6)
)

rfFit_stroke <- train(
  stroke ~ .,                   
  data = trainData_ready, 
  method = "rf", 
  trControl = ctrl,
  tuneGrid = rf_grid,
  ntree = 100,
  trace = FALSE
)

# Print the best parameters and performance metrics
plot(rfFit_stroke)

# Best tuning parameters
optimal_rf_tune <- rfFit_stroke$bestTune
print(paste('Best Size and Decay tuning parameters for Random Forest:', paste(optimal_rf_tune, collapse = ',')))
```

## Results - Summary Table

```{r SumTable}

# 1.Make predictions for Linear Discriminant Analysis
lda_pred <- predict(ldaFit_stroke, newdata = testData_ready)
lda_cm <- confusionMatrix(lda_pred, testData_ready$stroke)

# 2.Make predictions for Penalized Logistic Regression
plr_pred <- predict(plrFit_stroke, newdata = testData_ready)
plr_cm <- confusionMatrix(plr_pred, testData_ready$stroke)

# 3.Make predictions for Nearest Shrunken Centroids
nsc_pred <- predict(nscFit_stroke, newdata = testData_ready)
nsc_cm <- confusionMatrix(nsc_pred, testData_ready$stroke)

# 4.Make predictions for Neural Network
nn_pred <- predict(nnFit_stroke, newdata = testData_ready)
nn_cm <- confusionMatrix(nn_pred, testData_ready$stroke)

# 5.Make predictions for Random Forest
rf_pred <- predict(rfFit_stroke, newdata = testData_ready)
rf_cm <- confusionMatrix(rf_pred, testData_ready$stroke)

extract_accuracy <- function(model, cm) {
  tibble(
    model = model,
    Accuracy = cm$overall['Accuracy'],
    `CI Lower` = cm$overall['AccuracyLower'],
    `CI Upper` = cm$overall['AccuracyUpper']
  )
}

# Combine accuracies into a table
accuracies <- bind_rows(
  extract_accuracy('Linear Discriminant Analysis', lda_cm),
  extract_accuracy('Penalized Logistic Regression', plr_cm),
  extract_accuracy('Nearest Shrunken Centroids', nsc_cm),
  extract_accuracy('Neural Network', nn_cm),
  extract_accuracy('Random Forest', rf_cm)
)

# Display the table using gt package
accuracies %>%
  arrange(-Accuracy) %>%
  gt() %>%
  fmt_number(columns = c(Accuracy, `CI Lower`, `CI Upper`), decimals = 3)

```

### Tuned Models

```{r TunedModel}
set.seed(rseed)
# Penalized Logistic Regression with optimal parameters
plrFit_final <- train(stroke ~ ., 
                       data = trainData_ready,
                       method = 'glmnet',
                       preProc = c('center', 'scale'),
                       metric = 'ROC',
                       trControl = ctrl,
                       tuneGrid = expand.grid(
                         alpha = optimal_plr_tune$alpha,
                         lambda = optimal_plr_tune$lambda
                       ))
set.seed(rseed)
# Retrain NSC with optimal parameters
nscFit_final <- train(stroke ~ ., 
                        data = trainData_ready,
                        method = 'pam',
                        preProc = c('center', 'scale'),
                        metric = 'ROC',
                        trControl = ctrl,
                        tuneGrid = expand.grid(
                          threshold = optimal_nsc_tune$threshold
                        ))
set.seed(rseed)
# Retrain NN with optimal parameters
nnFit_final <- train(stroke ~ .,
                       data = trainData_ready,
                       method = 'nnet',
                       maxit = 500,
                       preProcess = c('center', 'scale'),  
                       metric = 'ROC',    
                       trControl = ctrl,
                       tuneGrid = expand.grid(
                         size = optimal_nn_tune$size,
                         decay = optimal_nn_tune$decay
                         ),
                       trace = FALSE)
set.seed(rseed)
# Retrain RF with optimal parameters
rfFit_final <- train(stroke ~ .,
                      data = trainData_ready, 
                      method = "rf", 
                      ntree = 100,
                      trControl = ctrl,
                      tuneGrid = expand.grid(
                         mtry = optimal_rf_tune$mtry
                         ))

```

### Summary for Tuned Models

```{r FinalSum}
set.seed(rseed)
# Make predictions for Linear Discriminant Analysis
lda_pred <- predict(ldaFit_stroke, newdata = testData_ready)
lda_cm2 <- confusionMatrix(lda_pred, testData_ready$stroke)

# Make predictions for Penalized Logistic Regression
plr_pred2 <- predict(plrFit_final, newdata = testData_ready)
plr_cm2 <- confusionMatrix(plr_pred2, testData_ready$stroke)

# Make predictions for Nearest Shrunken Centroids
nsc_pred2 <- predict(nscFit_final, newdata = testData_ready)
nsc_cm2 <- confusionMatrix(nsc_pred2, testData_ready$stroke)

# Make predictions for Neural Network
nn_pred2 <- predict(nnFit_final, newdata = testData_ready)
nn_cm2 <- confusionMatrix(nn_pred2, testData_ready$stroke)

# Make predictions for Random Forest
rf_pred2 <- predict(rfFit_final, newdata = testData_ready)
rf_cm2 <- confusionMatrix(rf_pred2, testData_ready$stroke)


extract_accuracy <- function(model, cm) {
  tibble(
    model = model,
    Accuracy = cm$overall['Accuracy'],
    `CI Lower` = cm$overall['AccuracyLower'],
    `CI Upper` = cm$overall['AccuracyUpper']
  )
}

# Combine accuracies into a table
accuracies <- bind_rows(
  extract_accuracy('Linear Discriminant Analysis', lda_cm2),
  extract_accuracy('Penalized Logistic Regression', plr_cm2),
  extract_accuracy('Nearest Shrunken Centroids', nsc_cm2),
  extract_accuracy('Neural Network', nn_cm2),
  extract_accuracy('Random Forest', rf_cm2),
)

# Display the table using gt package
accuracies %>%
  arrange(-Accuracy) %>%
  gt() %>%
  fmt_number(columns = c(Accuracy, `CI Lower`, `CI Upper`), decimals = 3)
```
